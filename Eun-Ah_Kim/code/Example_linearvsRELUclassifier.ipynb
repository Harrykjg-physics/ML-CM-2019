{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from http://cs231n.github.io/neural-networks-case-study/ (A. Karpathy)\n",
    "# by Y. Zhang and P. Ginsparg\n",
    "\n",
    "n = 200 # number of points per class\n",
    "k = 4 # number of classes\n",
    "d = 2\n",
    "X = [[],[]] # 2d coords\n",
    "\n",
    "np.random.seed(0)\n",
    "for j in range(k): #classes\n",
    "  r = np.linspace(0.0,1,n) # radius\n",
    "  theta = np.linspace(j*2*np.pi/k, (j+3.5)*2*np.pi/k, n) + .2*np.random.randn(n) # .84*(2pi radians) = 301degs\n",
    "  X[0] += list(r*np.cos(theta))\n",
    "  X[1] += list(r*np.sin(theta))\n",
    "\n",
    "X = np.array(X).T                         #nk x d array of data\n",
    "y = [i for j in range(k) for i in [j]*n]  #nk list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(*X.T, c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.axis([-1,1,-1,1])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n",
    "\n",
    "def do_plot(ax, hidlayer=True, tsleep=0):\n",
    "    # plot resulting classifier\n",
    "    Z = np.c_[xx.ravel(), yy.ravel()].dot(W) + b\n",
    "    if hidlayer: Z = np.maximum(0, Z).dot(W2) + b2          \n",
    "    Z = Z.argmax(1).reshape(xx.shape)\n",
    "    \n",
    "    ax.cla()\n",
    "    plt.title('iteration {}, loss {:.2f}'.format(i,loss))\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    ax.scatter(*X.T, c=y, s=40, cmap=plt.cm.Spectral)\n",
    "    ax.axis([-1,1,-1,1])\n",
    "    ax.axis('off')\n",
    "    display.display(fig) \n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(tsleep) #was .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"linear.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Linear Classifier, hyperparameters\n",
    "step_size = 1e-1\n",
    "reg = 1e-4 # regularization\n",
    "\n",
    "n_samples = len(X)  #800 = N*K\n",
    "b = np.zeros(k)\n",
    "# random initializations\n",
    "W = 0.01 * np.random.randn(d,k)\n",
    "\n",
    "fig=plt.figure(figsize=(6,6))\n",
    "\n",
    "for i in range(501): #gradient descent loop\n",
    "  \n",
    "  scores = X.dot(W) + b  # evaluate class scores, [N x K]\n",
    "  probs = np.exp(scores)\n",
    "  probs /= probs.sum(1, keepdims=True) # N*k = 200*4 logistic class probabilities\n",
    "  \n",
    "  correct_logprobs = -np.log( probs[range(n_samples), y] )\n",
    "  loss = correct_logprobs.sum() / n_samples  # average logistic loss\n",
    "  loss += 0.5*reg*(W*W).sum()                # plus regularization loss\n",
    "\n",
    "  if i % 25 == 0: do_plot(plt.gca(), False, .5)\n",
    "\n",
    "  # compute gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(n_samples),y] -= 1\n",
    "  dscores /= n_samples\n",
    "  \n",
    "  # backpropagate gradient to parameters (W,b)\n",
    "  dW = X.T.dot(dscores)\n",
    "  db = dscores.sum(0)\n",
    "  dW += reg*W # regularization gradient\n",
    "\n",
    "  # parameter update\n",
    "  W -= step_size * dW\n",
    "  b -= step_size * db\n",
    "            \n",
    "# evaluate training set accuracy\n",
    "accuracy = (scores.argmax(1) == y).mean()\n",
    "plt.title('Linear: {} iterations, training accuracy: {}'.format(i, accuracy));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"non-linear.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with hidden layer, hyperparameters\n",
    "step_size = 2e-1\n",
    "reg = 2e-4 # regularization\n",
    "\n",
    "h = 100 # size of hidden layer\n",
    "b = np.zeros(h)\n",
    "b2 = np.zeros(k)\n",
    "# random initializations\n",
    "W = 0.01 * np.random.randn(d,h)\n",
    "W2 = 0.01 * np.random.randn(h,k)\n",
    "\n",
    "fig=plt.figure(figsize=(6,6))\n",
    "plt.axis('off')\n",
    "\n",
    "n_samples = len(X)\n",
    "\n",
    "for i in range(10001): # gradient descent loop\n",
    "  \n",
    "  hL = np.maximum(0, X.dot(W) + b) # hidden layer ReLU activation\n",
    "  scores = hL.dot(W2) + b2         # class scores, [n x k]\n",
    "  \n",
    "  # compute the class probabilities, logistic\n",
    "  probs = np.exp(scores)\n",
    "  probs /= probs.sum(1, keepdims=True)\n",
    "  \n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "  correct_logprobs = -np.log(probs[range(n_samples),y])\n",
    "  loss = correct_logprobs.sum() / n_samples  #data loss\n",
    "  loss += 0.5*reg*(W*W).sum() + 0.5*reg*(W2*W2).sum() #plus regularization loss\n",
    "\n",
    "  if i % 100 == 0: do_plot(fig.gca())\n",
    "    \n",
    "  # compute the gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(n_samples),y] -= 1\n",
    "  dscores /= n_samples\n",
    "  \n",
    "  db2 = dscores.sum(0)              # backprop to parameters W2 and b2\n",
    "  dW2 = hL.T.dot(dscores) + reg*W2  # with regularization gradient\n",
    "\n",
    "  dhL = dscores.dot(W2.T)           # backprop to hidden layer\n",
    "  dhL[hL <= 0] = 0                  # backprop the ReLU non-linearity\n",
    "  dW = X.T.dot(dhL) + reg*W\n",
    "  db = dhL.sum(0)\n",
    "  \n",
    "  # parameter updates\n",
    "  W  -= step_size * dW\n",
    "  b  -= step_size * db\n",
    "  W2 -= step_size * dW2\n",
    "  b2 -= step_size * db2\n",
    "\n",
    "accuracy = (scores.argmax(1) == y).mean()  #training set accuracy\n",
    "plt.title('\"Deep\": {} iterations, training accuracy: {:.2f}'.format(i, accuracy));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
